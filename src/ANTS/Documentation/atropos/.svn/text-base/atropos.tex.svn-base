%\documentclass[11pt,english]{article}
%
%% Set page margins correctly
%\usepackage{geometry}
%%\geometry{letterpaper,top=0.5in,left=0.5in,bottom=0.5in,top=0.5in,headsep=6pt,footskip=18pt}
%\geometry{letterpaper,top=0.5in,left=0.5in,bottom=0.5in,top=0.5in,headsep=6pt,footskip=18pt}
%
%% Use fancy header style
%\usepackage{fancyhdr}
%\pagestyle{fancy}
%\renewcommand{\headrulewidth}{0.75pt}
%\renewcommand{\footrulewidth}{0.75pt}
%\lhead{\footnotesize Principal Investigator/Program Director (Last, First, Middle): }
%\rhead{Yushkevich, Paul A.} \lfoot{\footnotesize PHS 398/2590 (Rev. 04/06)} \rfoot{\footnotesize
%\textbf Continuation Format Page} \cfoot{\footnotesize Page \underline{~~\thepage~~}}
%
%% Use pslatex fonts
%\usepackage{pslatex}
%\renewcommand{\familydefault}{\sfdefault}
%\renewcommand{\baselinestretch}{.9}
%




% START JUNK
\documentclass[11pt,english]{article}

% Set page margins correctly
\usepackage{geometry}
\usepackage{url}
\geometry{letterpaper,top=1.0in,left=1.0in,bottom=1.0in,top=1.0in,headsep=6pt,footskip=18pt}

\usepackage{lscape}
\usepackage[square,comma,numbers,sort&compress]{natbib}

% Use fancy header style
\usepackage{fancyhdr}
\pagestyle{empty}
\renewcommand{\headrulewidth}{0.75pt}
\renewcommand{\footrulewidth}{0.75pt}
%\lhead{\footnotesize Principal Investigator/Program Director (Last, First, Middle): }
%\rhead{Gee, James C.} \lfoot{\footnotesize PHS398 (06/07)} \rfoot{\footnotesize
%\textbf Continuation Format Page} \cfoot{\footnotesize Page \underline{~~\thepage~~}}

\usepackage{setspace}
\usepackage{listings}
\usepackage{float}


\floatstyle{plain}
\newfloat{command}{thp}{lop}
\floatname{command}{Command}

\doublespacing

% Use pslatex fonts
%\usepackage[T1]{fontenc}
%\usepackage{mathptmx}


% END JUNK

% All other packages
\usepackage{boxedminipage}
\usepackage{graphicx}
\usepackage{amsmath,amsfonts}
\usepackage{babel,verbatim}
\usepackage{enumerate}
%\usepackage{dsfont}
\usepackage{longtable}
\usepackage{multirow}
\usepackage{sectsty}
\usepackage[compact]{titlesec}
\usepackage[usenames]{color}
\usepackage{ulem}
\usepackage{multirow,booktabs,ctable,array}

%GATHER{../../../shared/bibtex/biblio.bib}
%GATHER{../../../shared/bibtex/brainmri.bib}
%GATHER{../../../shared/bibtex/registration.bib}
\graphicspath{{./Figures/}
                          }

%\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand{\argmax}{\operatornamewithlimits{argmax}}
\newcommand{\argmin}{\operatornamewithlimits{argmin}}

\long\def\symbolfootnote[#1]#2{\begingroup%
\def\thefootnote{\fnsymbol{footnote}}\footnote[#1]{#2}\endgroup}

    \usepackage{color}

    \definecolor{listcomment}{rgb}{0.0,0.5,0.0}
    \definecolor{listkeyword}{rgb}{0.0,0.0,0.5}
    \definecolor{listnumbers}{gray}{0.65}
    \definecolor{listlightgray}{gray}{0.955}
    \definecolor{listwhite}{gray}{1.0}

\newcommand{\lstsetcpp}
{
\lstset{frame = tb,
        framerule = 0.25pt,
        float,
        fontadjust,
        backgroundcolor={\color{listlightgray}},
        basicstyle = {\ttfamily\scriptsize},
        keywordstyle = {\ttfamily\color{listkeyword}\textbf},
        identifierstyle = {\ttfamily},
        commentstyle = {\ttfamily\color{listcomment}\textit},
        stringstyle = {\ttfamily},
        showstringspaces = false,
        showtabs = false,
        numbers = none,
        numbersep = 6pt,
       numberstyle={\ttfamily\color{listnumbers}},
        tabsize = 2,
        language=[ANSI]C++,
        floatplacement=!h,
        caption={\small \baselineskip 12pt Atropos short command line menu which is invoked using the `{\ttfamily -h}' option. 
        The expanded menu, which provides details regarding the possible parameters and usage 
        options, is elicited using the `{\ttfamily {-}{-}help}' option.},
        captionpos=b,
        label=listing:command
        }
}

\newcommand{\lstsetcppnfour}
{
\lstset{frame = tb,
        framerule = 0.25pt,
        float,
        fontadjust,
        backgroundcolor={\color{listlightgray}},
        basicstyle = {\ttfamily\scriptsize},
        keywordstyle = {\ttfamily\color{listkeyword}\textbf},
        identifierstyle = {\ttfamily},
        commentstyle = {\ttfamily\color{listcomment}\textit},
        stringstyle = {\ttfamily},
        showstringspaces = false,
        showtabs = false,
        numbers = none,
        numbersep = 6pt,
        numberstyle={\ttfamily\color{listnumbers}},
        tabsize = 2,
        language=[ANSI]C++,
        floatplacement=!h,
        caption={\small \baselineskip 12pt N4 short command line menu which is invoked using the `{\ttfamily -h}' option. 
        The expanded menu, which provides details regarding the possible parameters and usage 
        options, is elicited using the `{\ttfamily {-}{-}help}' option.  },
        captionpos=b,
        label=listing:n4
        }
}



%\renewcommand{\topfraction}{0.85}
%\renewcommand{\textfraction}{0.1}
%\renewcommand{\floatpagefraction}{0.75}

% Different font in captions
%\newcommand{\captionfonts}{\small}
%\makeatletter  % Allow the use of @ in command names
%\long\def\@makecaption#1#2{%
%  \vskip\abovecaptionskip
%  \sbox\@tempboxa{{\captionfonts #1: #2}}%
%  \ifdim \wd\@tempboxa >\hsize
%    {\captionfonts #1: #2\par}
%  \else
%    \hbox to\hsize{\hfil\box\@tempboxa\hfil}%
%  \fi
%  \vskip\belowcaptionskip}
%\makeatother   % Cancel the effect of \makeatletter

%Automated Segmentation of Ventilation Defects on $^3$He Lung MRI}
%\date{}
%\author{ Nicholas J. Tustison, DSc,$^{1*}$ Talissa A. Altes, MD,$^2$ Eduard E. de Lange, MD,$^2$
%  Brian B. Avants, PhD, $^1$ John P. Mugler III, PhD,$^2$ and James C. Gee, PhD,$^1$ \\
%  $^1$ Penn Image Computing and Science Laboratory, University of Pennsylvania, Philadelphia, Pennsylvania,  USA.\\
%  $^2$ Department of Radiology, University of Virginia, Charlottesville, Virginia, USA}
%\address{3600 Market Street, Suite 370 \\
%                 Philadelphia, PA  19104\\
%                 tustison@picsl.upenn.edu}
%\advisor{James Gee}
\begin{document}

\normalem

\vspace*{5cm}

\begin{center}
{\Large \bf An Open Source Multivariate Framework for $n$-Tissue
  Segmentation with Evaluation on Public Data} \\
\vspace*{0.5cm}
{\normalsize Brian B.~Avants$^{1*}$,  Nicholas J.~Tustison$^2$%
\symbolfootnote[1]{
  The first two authors contributed equally to this work.
}, 
Jue Wu$^1$, Philip A.~Cook$^1$, and James C.~Gee$^1$} \\
\begin{singlespace} 
{\scriptsize  $^1$ Penn Image Computing and Science Laboratory, University of Pennsylvania, Philadelphia, Pennsylvania,  USA.\\
  $^2$ Department of Radiology, University of Virginia, Charlottesville, Virginia, USA}
\end{singlespace}
\end{center}

\vfill

\begin{singlespace} 
\scriptsize
\flushleft
%\line(1, 0){250} \\
{\bf Atropos:  $n$-Tissue Segmentation}\\
Corresponding author: \\
Brian B. Avants\\
3600 Market Street, Suite 370\\
Philadelphia, PA  19104\\
avants@picsl.upenn.edu\\
\end{singlespace} 

%\clearpage
%
%\vspace*{7cm}
%
%\begin{center}
%{\Large \bf An Open Source Framework for Multivariate $n$-Tissue Segmentation and Volumetric Cortical Parcellation:  Evaluation Using the BrainWeb, NIREP, and LPBA Data Sets}\\
%\end{center}

\clearpage


\begin{abstract} 
%Neuroanatomical coordinate systems are essential for the
%interpretation of structural and functional imaging studies.
%However, manual delineation of the sulco-gyral complex is time
%consuming and prone to variability given cortical complexity.  This
%work describes an open source, image-based approach to
%cortical parcellation which uses training data to propagate
%structural labelings to individual images.  Along with other
%neuroinformatics tools publicly distributed in our ANTs (Advanced Normalization Tools)%
We introduce {\em Atropos}, an ITK-based multivariate $n$-class
open source segmentation algorithm distributed
with ANTs.%
\footnote{
http://www.picsl.upenn.edu/ANTs
}  The Bayesian formulation of the segmentation problem is 
solved using the Expectation Maximization (EM) algorithm
with the modeling of the class intensities 
 based on either parametric or non-parametric finite mixtures.  
Atropos is capable of incorporating spatial prior probability maps,
(sparse) prior label maps and/or Markov Random Field (MRF) modeling.  Atropos has also been efficiently implemented to handle
large quantities of possible labelings (in the experimental section,
we use up to 69 classes) with a minimal memory footprint.  This work
describes the technical and implementation aspects of Atropos and
evaluates its performance on two different ground-truth datasets.
First, we use the BrainWeb dataset from Montreal Neurological
Institute to evaluate three-tissue segmentation performance via (1)
K-means segmentation without use of template data; (2) MRF
segmentation with initialization by prior probability maps derived from a group template; (3) Prior-based segmentation with use
of spatial prior probability maps derived from a group template.  We
also evaluate Atropos performance by using spatial priors to drive a 69-class EM segmentation
problem derived from the Hammers atlas from University College London.  These evaluation studies, combined with
illustrative examples that exercise Atropos options, demonstrate both
performance and wide applicability of this new platform-independent 
open source segmentation tool.
%Component (3) corresponds to the most
%realistic clinical case, while component (2) eliminates the confound
%of defining the cortex itself and focuses only on the parcellation
%problem.  Consistent with the open source dissemination of ANTs, we
%make publicly available all scripts and code from the evaluation.
\end{abstract}

%\begin{keyword}
%segmentation \sep expectation maximization \sep spatial prior \sep brain 
%\end{keyword}

\clearpage

\section{Introduction} As medical image acquisition technology has advanced,
significant investment has been made towards adapting classification
techniques for neuroanatomy.  Early work appropriated NASA
satellite image processing software for statistical classification of
head tissues in 2-D MR images \citep{Vannier1985}.  A proliferation of
techniques ensued with increasing sophistication in both core
methodology and degree of refinement for specific problems.  The
chronology of progress in segmentation may be tracked through both
technical reviews
\citep{Bezdek1993,Pal1993,Clarke1995,Pham2000,Viergever2001,Suri2002,Duncan2004,Balafar2010}
and evaluation studies
\citep[e.g.][]{Cuadra2005,Zaidi2006,Klauschen2009,Boer2010}.

The problem of accurately delineating the white matter, grey matter
and cerebrospinal fluid (and subdivisions) of the human brain continuously spurs
technical development in segmentation.  Following \cite{Vannier1985},
many researchers adopted statistical methods for $n$-tissue anatomical
brain segmentation.  The Expectation-Maximization (EM) framework is
natural \citep{Dempster1977} given the ``missing data'' aspect of this
problem.  The work described in \cite{Wells1996} was one of the first to use EM for
finding a locally optimal solution by iterating between bias field
estimation and tissue segmentation.  A core component of this work was
explicit modeling of the tissue intensity values as normal
distributions \citep{Cline1990} for both 2-D univariate simulated data
and T1 coronal images, which continues to find utility in contemporary
developments.  A secondary component was an extended non-parametric
probability model, also influenced by earlier work \citep{Kikinis1992}, where
Parzen windowing is used to model the tissue intensity distribution
omitting consideration of the underlying bias field.  Although
technically not an EM-based algorithm, the robustness of the latter has motivated its continued use even more recently \citep[e.g.][]{Weisenfeld2009}.

Subsequent development included the use of Markov Random Field (MRF)  modeling \citep{Geman1984}
to regularize the classification results \citep{Held1997} with later work adding heuristics concerning neuroanatomy to prevent 
over-regularization and the resulting loss of fine structural details \citep{Leemput1999,Leemput1999a}.  
A more formalized integration of generic MRF spatial priors was employed in the work of \cite{Zhang2001}, 
commonly referred to as FAST (FMRIB's Automated Segmentation Tool), which is in widespread use
given its public availability and good performance.  More recently, a uniform distribution of local MRFs within the brain volume and their subsequent integration into a global solution has been proposed obviating the need for an 
explicit bias correction solution \citep{Scherrer2009}.  

Several initialization strategies have been proposed to overcome the characteristic
susceptibility of EM algorithms to local optima.   Common low-level initialization steps 
include uniform probability assignment \citep{Wells1996},
Otsu thresholding \citep{Zhang2001}, and K-means clustering
\citep{Pappas1992}.  More sophisticated low-level initialization
schemes include that of \cite{Greenspan2006} in which a dense spatial
distribution of Gaussians is used to capture the complex
neuroanatomical layout with subsequent processing used to conjoin
subsets of such Gaussians belonging to the same tissue classes.
Recently, reseachers have begun to rely on spatial prior probability
maps of anatomical structures of interest to encode domain knowledge\citep{Leemput1999a,Marroquin2002,Ashburner2005}.  These spatial prior
probability maps may also provide an initial segmentation.
Related technological developments model partial volume effects for
increased accuracy in brain segmentation
\citep{Ruan2000,Ballester2002,Leemput2003}.

A general trend towards more integrative neuroanatomical image processing led to the work described in
\cite{Ashburner2005} which is publicly available within SPM5, a
large-scale Matlab module in which registration, segmentation, and
bias field correction can be simultaneously modeled within a single
optimization scheme. The roots of this very popular software package
stem back to early work by Karl Friston which laid the basis for
statistical parametric mapping \citep{Friston1990}. 
 Similar integrative brain processing was provided in \cite{Pohl2006} in which
segmentation and registration parameters were optimized simultaneously
while casting the inhomogeneity model parameters of \cite{Wells1996}
as nuisance variables.  Continued work involved recursive parcellation
of the brain volume by considering sub-structures in a hierarchical
manner \citep{Pohl2007}.  An implementation is provided in 3D slicer
\citep{Pieper2006}---an open source medical image computation and
visualization package with developmental
contributions from multiple agencies including both private and
academic institutions.

Related neuroanatomical research concerns the selection of geometric
features of the cortex \citep[e.g.][]{Goualher1999} which aims at
understanding the functional-anatomical relationship of the human
brain. Recent endeavors produce a dense cortical labeling in which
every point of the cortex is classified, i.e. a cortical parcellation
\citep{Fischl2004,Heckemann2006,Destrieux2010}.  Various techniques
have been proposed to reduce the manual effort required to densely
label a high-resolution neuroimage; one example is the popular
software package known as Freesurfer
\citep{Dale1999,Fischl1999,Fischl2004}.  In contrast to the volumetric
approach detailed in this work, Freesurfer is primarily a
surface-based technique in which the brain structures such as the
grey-white matter interface and pial surfaces are processed, analyzed,
and displayed as tessellated surfaces \citep{Dale1999,Fischl1999}.
Advantages of surface representations include the ability to map
processed neuroanatomy to simple geometric primitives such as spheres
or planes and the ease of including topological constraints in the
analysis workflow.  These types of methods, including Klein's
Mindboggle \citep{Klein2005}, would usually follow an initial
segmentation by a volumetric method such as Atropos.

%{\color{red}{{\em Needs work}:  Mindboggle, Heckemann  One advantage
%of the pipeline we propose is that we parcellate cortex entirely in
%the image space, thus avoiding the difficulty of transferring labels
%from the mesh space back to the image space---a problem that is a confound of surface-based methods\citep{Klein2010}.}}

Researchers in aging often focus on
accurately segmenting the T1 MRI of elderly controls and subjects
suffering from neurodegeneration, for instance, via SIENA \citep{Smith2007}.  A recent evaluation study 
compared kNN segmentation, SPM Unified Segmentation and SIENA and
found different performance
characteristics under different evaluation criteria
\citep{Bresser2011}.  \cite{Klauschen2009} had similar findings when
comparing SPM5, FSL and FreeSurfer.  These studies suggest that no
single method performs best under every measurement and, along with the No
Free Lunch theorem \citep{Wolpert1997}, highlight the need for segmentation tools that are tunable for different
problems and research goals.  

Our open source segmentation tool, which we have dubbed {\em
Atropos},%
\footnote{Atropos is one of the three Fates from Greek
mythology characterized by her dreaded shears used to decide the
destiny of each mortal.  Also, consistent with the entomological motif
of our ANTs, {\it Acherontia atropos} is a species of large
moth known for the skull-like pattern visible on its thorax.  }
efficiently and flexibly implements an $n$-tissue paradigm for
voxel-based image segmentation.  Atropos allows users to harness its
generalized EM algorithm for standard tissue classification of
the brain into gray matter, white matter and cerebrospinal fluid even
in cases of multivariate image data---relevant when more than one 
view of anatomy aids segmentation, as in neonatal brain tissue classification
\citep[e.g.][]{Prastawa2005,Weisenfeld2009}.  
Atropos equally allows incarnations that use EM to simultaneously
maximize the posterior probabilities of many classes with minimal
random access memory requirements, for instance,
when parcellating the brain into hemispheres, cortical regions and
deep brain structures such as amygdala, hippocampus and
thalamus.  Atropos contains features of its predecessors for
performing $n$-tissue segmentation including imposition of prior
information in the form of MRFs and template-based spatial prior
probability maps as well as weighted combinations of these terms.  We
also borrow an idea from \cite{Boykov2004} and use sparse spatial
priors to provide initialization and boundary conditions for Atropos
EM segmentation in a semi-interactive manner.  In short, Atropos seeks
to provide a segmentation toolbox that may be modified,
tuned and refined for different use scenarios.

Coupled with the registration \citep{Avants2011} and template building
\citep{Avants2010} already included in the ANTs, Atropos is a
versatile and powerful software tool which touches multiple aspects of
our brain processing pipeline.  We use Atropos to address brain
extraction \citep{Avants2010a}, grey matter/white matter/cerebrospinal
fluid segmentation, label fusion/propagation and cortical
parcellation.  We also allow Atropos to interact with the recently
developed N4 bias correction software \citep{Tustison2010} in an
adaptive manner.  To further highlight the value of this open source
contribution, we performed a search of software attributes on NITRC
and found that as of November 2010 no stand-alone EM methods are
currently listed.  We also evaluate Atropos performance on two brain
MRI segmentation objectives.  First, three-tissue classification.
Second, we test our ability to parcellate the brain into 69
neuroanatomical regions to illustrate the practical value of the
low-memory implementation within this paper.  Although Atropos may be
applied to multivariate data from arbitrary modalities, we limit our
evaluation to tissue classification in T1 neuroimaging in part due to
the abundance of ``gold-standard'' data for this modality.  Consistent
with our advocacy of open science (not to mention the facilitation of
analysis due to accessibility) we also only use publicly available
data sets.  For this reason, all results in this paper are
reproducible with the caveat that users may require some guidance from
the authors or other users in the community.

Organization of this work is as follows: we first describe the theory
behind the various components of Atropos while acknowledging that more
theoretical discussion is available elsewhere.  This is followed by a
thorough discussion of implementation which, though often overlooked,
is of immense practical utility.  We then report results on the
BrainWeb and Hammers dataset.  Finally, we
provide a discussion of our results and our open source contribution
in the context of the remainder of this paper and of previous and
future work.

\section{Theoretical Foundations for Atropos Segmentation} 

Atropos encodes a family of Bayesian segmentation techniques that may
be configured in an application-specific manner.  The theory
underlying Atropos dates back 20$+$ years and is representative of
some of the most innovative work in the field.  Although we summarize
some of the theoretical work in this section, we recommend that 
the interested reader consult the deep literature in this field for
additional perspective and proofs behind the major concepts. 

Bayes' theorem provides a powerful mechanism for making inductive
inferences assuming the availability of quantities defining the
relevant conditional probabilities, specifically the likelihood and
prior probability terms.  Bayesian paradigms for brain image
segmentation employ a user selected observation model defining the
likelihood term and one or more prior probability terms.  The product
of likelihood(s) and prior(s) is proportional to the posterior
probability.  The likelihood term has been previously defined both 
parametrically (e.g. a Gaussian model) and non-parametrically
(e.g. Parzen windowing of the sample histogram).  The prior term, as
given in the literature, has often been formed either as
MRF-based and/or template-based.  An image segmentation solution in this
context is an assignment of one label to each voxel%
\footnote{
In the classic 3-tissue segmentation case, each voxel in the brain region is assigned a label of `cerebrospinal fluid (csf)', `gray matter (gm)', or `white matter (wm)'. 
}
such that the posterior probability is maximized.  The next sections
introduce notation and provide a formal description of three essential
components in Bayesian segmentation, viz.
\begin{itemize}
  \item the likelihood or observation model(s),
  \item the prior probability quantities derived from a generalized
    MRF and template-based prior terms, and 
%, and a novel distance prior used for label propagation, and 
  \item the optimization framework for maximizing the posterior probability.
\end{itemize}
These components are common across most EM segmentation algorithms. 

\subsection{Notation}
Assume a field, $\mathcal{F}$, whose values are known at discrete
locations, i.e. sites, within a regular voxel lattice that makes up an
image domain, $\mathcal{I}$.  Note that $\mathcal{F}$ can be a scalar
field in the case of unimodal data (e.g. T1 image only) or a vector
field in the case of multimodal data (e.g. T1, T2, and proton density
images).  A specific set of observed values, denoted by $\mathbf{y}$,
are indexed at $N$ discrete locations in $\mathcal{I}$ by $i \in \{1,
2, \ldots, N\}$.  This random field, $Y = \{y_1, y_2, \ldots, y_N \}$,
serves as a discrete representation of an observed image's intensities.  A labeling
of this image, also known as a hard segmentation, assigns to each site
in $\mathcal{I}$ one of $K$ labels from the finite set $\mathcal{L} =
\{l_1, l_2, \ldots, l_K\}$.  Also considered a random field, this
discrete labeling is $X = \{x_1, x_2, \ldots, x_N\}$ where each
$x_i \in \mathcal{L}$.   We use $\mathbf{x}$ to denote a specific set of labels in
$\mathcal{I}$ and a valid, though not necessarily optimal, solution to the segmentation problem.

%Furthermore, it is assumed that $\mathcal{F}$ is a random field whose true state, $\mathcal{S}$, is `hidden' beneath its observed values but fully represented by $\mathcal{L}$.%
%\footnote{
%For example, in the classic 3-tissue segmentation problem $\mathcal{L}$ is defined as the set $ \{\mathrm{csf}, \mathrm{gm}, \mathrm{wm}\}$ and each voxel in the brain region is assigned one and only one of these three labels.  
%}

\subsection{Segmentation Objective Function}

 Atropos optimizes a class
of user selectable objective functions each of which may
be represented in a generic Bayesian framework, as described by \cite{Sanjay-Gopal1998}.  This framework
requires {\em likelihood models} and {\em prior models} which enter
into Bayes' formula,
\begin{equation}\label{eq:bayes}
p(\mathbf{x}|\mathbf{y})=\underbrace{
p(\mathbf{y}|\mathbf{x})}_{\text{Likelihood(s)}} \underbrace{
p(\mathbf{x})}_{\text{Prior(s)}}\frac{1}{p(\mathbf{y})} 
\end{equation} 
where the normalization term, $1/\mathbf{y}$, is a constant that does
not affect the optimization \citep{Sanjay-Gopal1998}.
Given choices for likelihood models and prior probabilities, the Bayesian
segmentation solution is the labeling $\hat{\mathbf{x}}$ which
maximizes the posterior probability, i.e.
\begin{align} \hat{\mathbf{x}} = \argmax_{\mathbf{x}}
\left\{p(\mathbf{y}|\mathbf{x})p(\mathbf{x})\right\}.
\end{align} Similar to its predecessors, Atropos employs
the EM framework \citep{Dempster1977} to find maximum likelihood
solutions to this problem.  The following sections detail the 
Atropos EM along with choices for the likelihood and prior terms.  

\subsection{Likelihood or Observation Models}
To each of the $K$ labels corresponds a single probabilistic model describing the variation of $\mathcal{F}$ over $\mathcal{I}$.  We denote this set of $K$ likelihood models as $\Phi = \{p_1, p_2, \ldots, p_K\}$.  Using the standard notation, $\mathrm{Pr}(S=s) = p(s)$, $\mathrm{Pr}(S=s|T=t) = p(s|t)$, we can define these voxelwise probabilities, $\mathrm{Pr}_k( Y_i = y_i | X_i = l_k ) = p_k(y_i|l_k)$, in either parametric or non-parametric terms.   Given its simplicity and good performance, in the parametric case, $p_k$ is typically defined as a normal distribution, i.e.
\begin{align}\label{eq:param}
  p_k\left(y_i|l_k\right) &= G\left(\mu_k;\sigma_k\right) \nonumber \\
                    &= \frac{1}{\sqrt{2\pi \sigma_k^2}}\exp\left( \frac{ -(y_i - \mu_k)^2 }{2\sigma_k^2} \right)
\end{align}
where the parameters $\mu_k$ and $\sigma_k^2$ respectively represent
the mean and variance of the $k^{th}$ model.  When $y_i$ is a vector
quantity, we replace the Euclidean distance by Mahalanobis distance
and define multivariate Gaussian parameters via a mean vector,
$\boldsymbol{\mu}_k$, and covariance matrix, ${\bf \Sigma}_k$.

A common technique for the non-parametric variant is to define $p_k$ using Parzen windowing of the sample observation histogram of $\mathbf{y}$, i.e.
\begin{align} \label{eq:nonparam}
  p_k\left(y_i|l_k\right) &= \frac{1}{N_B} \sum_{j=1}^{N_B} \frac{1}{\sqrt{2\pi \sigma_j^2}}\exp\left( \frac{ -(y_i - c_j)^2 }{2\sigma_j^2} \right)
\end{align}
where $N_B$ is the number of bins used to define the histogram of the
sample observations (in Atropos the default is $N_B = 32$) and $c_j$
is the center of the $j^{th}$ bin in the histogram.  $\sigma_j$ is the width of each of the $N_B$ Gaussian kernels.  For multi-modal data in which the number of components of $y_i$ is greater than one, a Parzen window function is constructed for each component.  The likelihood value is determined by the joint probability given by their product.

Atropos segmentation likelihood estimates are based on the classical finite mixture model (FMM).
FMM assumes independency between voxels to calculate the probability
associated with the entire set of observations, $\mathbf{y}$.  Spatial
interdependency between voxels is modeled by the prior probabilities
discussed in the next section.  Marginalizing over the set of possible labels, $\mathcal{L}$, leads to the following probabilistic formulation
\begin{align}\label{eq:likelihood}
  p(\mathbf{y}|\mathbf{x}) = \prod_{i=1}^N \left(      
                                                        \sum_{k=1}^K \gamma_k p_k(y_i|l_k)
                                                        \right)
\end{align}
where $\gamma_k$ is the mixing parameter \citep{Ashburner2005}.  


\subsection{Prior Probability Models}
By modeling $\mathcal{F}$ via the set of observation models $\Phi$,
this so called finite-mixture model could be used to produce a
labeling or segmentation \citep[e.g.][]{Wells1996}.  However, as
pointed out by \cite{Zhang2001}, exclusive use of the intensity profile produces a less than optimal
solution because spatial contextual considerations are ignored.  This
has been remedied by the introduction of a host of prior probability
models including those characterized by use of MRF theory and
template-based information.  For example, in the works of
\cite{Leemput1999a} and \cite{Weisenfeld2009}, the original global
prior term given in \cite{Wells1996} is replaced by the product of the
template-based and the MRF-based prior terms.  In addition to their
descriptions below, we discuss a third possible
prior/objective combination in the form of a (sparse) prior labeling which fixes specific points
of the segmentation and uses EM to propagate this information
elsewhere in the image.  
% extension of template-based Euclidean and geodesic distance label propagation.

\subsubsection{Generalized MRF Prior}
One may incorporate spatial coherence into the segmentation by
favoring labeling configurations in which voxel neighborhoods tend
towards homogeneity. This intuition is formally described by MRF
theory in which spatial interactions in voxel neighborhoods can be
modeled \citep{Li2001}.

We assume the random field introduced earlier, $X$, is an MRF characterized by a neighborhood system, $\mathcal{N}_i$, on the lattice, $\mathcal{I}$, composed of the neighboring sites of $i$.  This neighborhood system is both noninclusive, i.e. $i \notin \mathcal{N}_i$, and reciprocating, i.e. $i \in \mathcal{N}_j \Leftrightarrow j \in \mathcal{N}_i$.   As an MRF, $X$ also satisfies the positivity and locality conditions, i.e., 
 \begin{align} \label{eq:mrf}
  p(\mathbf{x}) > 0, \,\, \forall\mathbf{x} 
 \end{align}
and where $\mathbf{x}$ is any particular labeling configuration on $X$ (in
other words, any labeling permutation on $X$ is {\em a priori}
possible).  The MRF locality condition is then, 
\begin{align}\label{eq:cond}
  p\left(x_i | x_{\mathcal{I}-\{i\}}\right) = p\left(x_i |
    x_{\mathcal{N}_i}\right) 
\end{align}
where $x_{\mathcal{I}-\{i\}}$ is the labeling of the entire image lattice except at site $i$ and  $x_{\mathcal{N}_i}$ is the labeling of $\mathcal{N}_i$.  This locality property enforces solely local considerations based on the neighborhood system in calculating the probability of the particular configuration, $\mathbf{x}$.  Following these two assumptions, the Hammersley--Clifford theorem provides the basis for treating the MRF distribution (cf. Eqn. (\ref{eq:mrf})) as a Gibbs distribution \citep{Besag1974,Geman1984}, i.e.
\begin{align} \label{eq:gibbs}
p(\mathbf{x}) = Z^{-1} \exp\left(-U(\mathbf{x})\right) 
\end{align}
with $Z$ a normalization factor known as the {\em partition function}
and $U(\mathbf{x})$ the {\em energy function} which can take
several forms \citep{Li2001}. In Atropos, as is the case with many
other segmentation algorithms of the same family, we choose
$U(\mathbf{x})$ such that it is only composed of a sum over pairwise
interactions between neighboring sites across the image,%
\footnote{
Using a more expansive definition of  $U(\mathbf{x})$, 
\begin{align}
U(\mathbf{x}) = \sum_{i = 1}^N \left( V_i(x_i) + \beta \sum_{j \in
    \mathcal{N}_i} V_{ij}( x_i, x_j ) \right) \nonumber
\end{align}
would permit casting the other prior terms inside the definition of $U(\mathbf{x})$ in the form of the external field $V_i(x_i)$ but, for clarity purposes, we consider them separately. 
}
i.e.
\begin{align}\label{eq:U}
U(\mathbf{x}) = \beta \sum_{i = 1}^N \sum_{j \in \mathcal{N}_i} V_{ij}( x_i, x_j )
\end{align}
where $V_{ij}$ is typically defined in terms of the Kronecker delta,
$\delta_{ij}$, based on the classical Ising potential (also known as a Potts model) \citep{Besag1974}
\begin{align}
V_{ij}(x_i, x_j) &= \delta_{ij} \nonumber \\
                          &= \left\{
                          \begin{array}{ll}
                            0 & \text{if } x_i = x_j \\
                            1 & \text{otherwise}
                          \end{array}
                         \right.   
\end{align}
and $\beta$ is a granularity term which weights the contribution of
the MRF prior on the segmentation solution.  
Since Atropos allows for non-uniform neighborhood systems and systems in which not just the immediate face-connected neighbors are considered, we use the modified function also used in \cite{Noe2001}, which weights the interaction term by the Euclidean distance, $d_{ij}$,  between interacting sites $i$ and $j$ such that 
\begin{align}
  V_{ij} = \frac{\delta_{ij}}{d_{ij}}
\end{align}
so that sites in the neighborhood closer to $i$ are weighted more
heavily than distant sites.

\subsubsection{Template-Based Priors}
A number of researchers have used templates to both
ensure spatial coherence and incorporate prior knowledge in
segmentation.  A common technique is to select labeled subjects from a
population from which a template is constructed \citep[e.g.][which is
also available in ANTs]{Avants2010}.  Each labeling can
then be warped to the template where the synthesis of warped labeled
regions produces a prior probability map or prior label map encoding
the spatial distribution of labeled anatomy which can be harnessed in
joint segmentation/registration or Atropos/ANTs hybrids involving unlabeled subjects.

We employ the strategy given in \cite{Ashburner2005} in which the
stationary mixing proportions, $\mathrm{Pr}(x_i = l_k) = \gamma_k$
(cf. Eqn. (\ref{eq:likelihood})), 
describing the prior probability that label $l_k$ corresponds to a particular voxel, regardless of intensity, are replaced by the following spatially varying mixing proportions,
\begin{align}
\mathrm{Pr}(x_i = l_k) = \frac{\gamma_k t_{ik}}{\sum_{j=1}^K\gamma_j t_{ij}}.
\end{align}
The $t_{ik}$ is the prior probability value at site $i$ which was
mapped, typically by image registration, to the local image
from a template data set.  The user may also
choose mixing proportions equal to
\begin{align}
\mathrm{Pr}(x_i = l_k) = \frac{t_{ik}}{\sum_{j=1}^K t_{ij}}
\end{align}
via the command line interface to the posterior formulation.

\subsubsection{Supervised Semi-Interactive Segmentation}
Brain segmentation methods have relied on user interaction for many
years \citep{Lim1989,Julin1997,Freeborough1997a,Yushkevich2006}.
Atropos is capable of benefitting from user knowledge via an
initialization and optimization that depends upon a spatially
varying prior label image passed as input.  Rapid, sparse
labeling---with visualization provided by ITK-SNAP
(www.itksnap.org)---enables an interaction and execution processing
loop that can be critical to solving segmentation problems with
challenging clinical data in which automated approaches fail.  This
part of Atropos design is inspired by the interactive graph cuts
pioneered by \cite{Boykov2001} and which has spawned many follow-up
applications.  The Atropos prior label
image pre-specificies the segmentation results at a subset of the
spatial domain by fixing the priors and likelihood (and, thus, the
posterior) at a subset of $\mathcal{I}$ to be $1$ for the known label
and $0$ for each other label at the same site.  The user input
therefore not only initializes the optimization, but also gives boundary conditions
that influence the EM solution outside of the known sites.  While the
graph-based min-cut max-flow solution is globally optimal for two
labels, only locally optimal optimizers are available for 3 or more
classes.  Thus, in most practical applications, EM is a
reasonable and efficient alternative to Boykov's solution.
Furthermore, one may automate the initialization process.  
{\color{blue}{We provide this capability to allow the user to implement
    an interactive editing and segmentation loop.  The user may
    run Atropos with sparse manual label guidance, evaluate the
    results, update the manual labels and repeat until achieving the desired
    outcome.  This processing loop may be performed easily with, e.g., ITK-SNAP.}}
\begin{comment}
{
\subsubsection{Distance Prior for Label Propagation}

In order to provide a dense segmentation even in regions where the template-based prior probability is 0, we use the template-based prior label images or probability maps described previously to formulate an optional distance prior probability.
Consider a subset of $\mathcal{I}$, which we denote $R_k$, defined by $\mathrm{Pr}(x_i = l_k) > \mathrm{Pr}(x_i = l_j) \,\,\forall j \in K, j \neq k$ and $\mathrm{Pr}(x_i = l_k) > 0$. 
We also denote the boundary of $R_k$ as $\partial R_k$ and the `peak' as $\rho_{R_k}$ where $i$ and $j$ are sites in $R_k$:
\begin{align}
  \rho_{R_k} = \argmax_{i \in R_k} \left\{ \min_{j \in \partial {R_k}} d\left(i,j\right)\right\}.
\end{align}
In other words, the peak is an interior site (or multiple interior sites) in $R_k$ characterized as having the maximum distance of all sites to any corresponding closest point on the boundary of $R_k$.  We denote this max-min distance as $\Delta_k$.  This allows us to define the distance prior probability for label propagation as follows:
\begin{align}\label{eq:prop}
  \mathrm{Pr}(x_i = l_k) = \left\{
                                               \begin{array}{ll}
                                                \alpha_k \exp \left( -\frac{d(i, \partial R_k)}{\sigma_k} \right) & \text{if } i \text{ is outside the region } R_k \\
                                                \alpha_k + d\left(i, \partial R_k\right)\left(\frac{1.0 - \alpha_k}{\Delta_k}\right) & \text{otherwise}
                                                \end{array}
                                                \right.
\end{align}
The probability calculations are illustrated in
Fig.~\ref{fig:distancePrior} and were designed so that the most
interior point, or set of points, of region $R_k$ would have a
distance prior probability of 1.0 which would linearly decrease within
the region to the boundary with a user selected probability value of
$\alpha_k$.  Outside $R_k$, the distance prior exponentially decays
with a decay rate governed by another user selected parameter $\sigma_k$.  This permits a dense segmentation throughout the segmentation region of $\mathcal{I}$ even where the template prior probabilities are zero. 

\begin{figure}
\begin{center}
\begin{tabular}{c}
\includegraphics[width=150mm]{distancePrior.pdf}
\end{tabular}
\caption{\baselineskip 12pt \small Graphical representation of the distance prior probability calculation associated with label $l_k$.  This extension of the popular template-based prior probability permits a dense labeling with the user specified mask. }
\label{fig:distancePrior}
\end{center}
\end{figure}



A crucial matter, particularly for propagating labels in the cortical
regions, is the choice of distance function.  One of the advantages to
surface-based methods is that distances can be calculated along the
surface mesh, thus discriminating between proximal cortical surface
points and those points which are situated close together due to
cortical folding.  In Atropos, we provide two possibilities---(1) a
geodesic distance function using a fast marching construction
\citep{Osher1988} of the distance function with Atropos's masked
region of interest and (2) a Euclidean distance function using the algorithm described in \cite{Maurer2003}.  The differences in label propagation results are illustrated in Fig.~\ref{fig:label_propagation}.   

Figure~\ref{fig:neonate} illustrates the benefit of geodesic distance
priors in segmenting T2 MRI of the 
neonatal brain.  Geodesic distance priors may be used to reduce ambiguity at
tissue interfaces, where partial voluming leads to mixed intensity.  
In neonatal brain segmentation from T2 MRI, the
interface of gray matter (low intensity) and cerebrospinal fluid
(bright intensity) often matches that of white matter (medium intensity). 
However, the white matter class should never appear near the surface
of the brain.  Thus, the distance from the brain surface may be used
to disambiguate voxels such as these.  
In this case, the distance of a voxel from a known
or expected tissue location serves to differentiate it from classes
that have similar intensity signature.  The geodesic distance is therefore an
alternative to explicit models of partial volume \citep{Ruan2000,Ballester2002,Leemput2003}. 

\begin{figure}
\begin{center}
\begin{tabular}{ccc}
\includegraphics[width=50mm]{sparse_labels.png} &
\includegraphics[width=50mm]{prop_euclidean.png} &
\includegraphics[width=50mm]{prop_geodesic.png} \\
(a) & (b) & (c) \\
\end{tabular}
\caption{\baselineskip 12pt \small  Illustration of label propagation differences between the Euclidean and geodesic distances which
are particularly acute in sinuous structures such as the cortex.  (a) Given a mask, represented in white, and a sparse labeling,
Atropos can be used to propagate the labels in a dense manner throughout the masked region.  (b) This propagation can occur using Euclidean distances in the image space via an Euclidean distance transform which can cause labels to jump meaningful anatomical boundaries.   (c) Given the cortical geometry, a more intuitive approach would be to propagate the labelings in a geodesic manner using a product of the fast marching paradigm which can be used in a manner that respects anatomical boundaries.  }
\label{fig:label_propagation}
\end{center}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[width=0.9\textwidth]{sparse_labels.png} 
\caption{\baselineskip 12pt \small  Neonatal brain segmentation.  }
\label{fig:neonate}
\end{center}
\end{figure}

}
\end{comment}

\subsection{Optimization}
Atropos uses expectation maximization to find a locally optimal
solution for the user selected version of the Bayesian segmentation
problem (cf. Eqn. (\ref{eq:bayes})).
After initial estimation of the
likelihood model parameters, EM iterates between calculation of the
missing optimal labels $\hat{\mathbf{x}}$ and subsequent re-estimation of the model
parameters by maximizing the expectation of the complete data
log-likelihood (cf. Eqn. (\ref{eq:likelihood})).  
%\begin{align}
%\log( p(\mathbf{y}) ) = \sum_{i=1}^N \log \left( \sum_{k=1}^K \gamma_k  p(\mathbf{y}|l_k)  \right)
%\end{align}
The expectation maximization procedure is derived in various
publications including \cite{Zhang2001} which yields the optimal mean
and variance (or covariance), but sets the mixing parameter $\gamma_k$
as a constant.  The Atropos implementation estimates $\gamma_k$ at
each iteration, similar to \cite{Ashburner2005}.%
\footnote{
Due to the lack of parameters in the non-parametric approach, it is not technically an EM algorithm (as described in \cite{Wells1996}).  However, the same iterative maximization is applicable and is quite robust in practice as evidenced by the number of researchers employing non-parametric models (see the Introduction).
}  
When spatial coherence constraints are included as an MRF prior in Atropos, the optimal segmentation solution becomes intractable.%
\footnote{
Consider $N$ sites each with a possible $K$ labels for a total of $N^K$ possible labeling configurations. For large $K \gg 3$, exact optimization is even more intractable than for the traditional 3-tissue scenario.
}
Although many optimization techniques exist (see the introduction in 
\cite{Marroquin2002} for a concise summary of the myriad optimization
possibilities)---each with their characteristic
advantages and disadvantages in terms of computational complexity and
accuracy---Atropos uses the well-known {\em Iterated Conditional
  Modes} (ICM)  \citep{Besag1986} which is greedy, computationally
efficient and provides good performance.  The EM employed in
Atropos may therefore be written as a series of steps:
\begin{description}
\item[Initialization:] In all cases, the user defines the number of
classes to segment.  The simplest initialization is by the classic
K-means or Otsu thresholding algorithms with only the number of classes 
specified by the user.
Otherwise, the user must provide prior information for each class in
the form of either a single $n$-ary prior label image or a series of
prior probability images, one for each class.  The initialization also
provides starter parameters.
\item[Label Update (E-Step):] Given the initialization and fixed
model parameters, {\color{blue}{Atropos is capable of updating the current label 
estimates using either a {\em synchronous} or {\em asynchronous} scheme.
The former is characterized by iterating through the image and determining
which label maximizes the posterior probability without updating any
labels until all voxels in the mask have been visited at which point
all the voxel labels are updated simultaneously (hence the descriptor 
``synchronous'').  This option is specified with \texttt{--icm [0]}. 
However, unlike asynchronous schemes characteristic of ICM, synchronous 
updates lack convergence guarantees.  To determine the labeling 
which maximizes the posterior probability for the asynchronous option, 
an ``ICM code'' image is created once for all iterations by iterating 
through the image and assigning an ICM code label to each voxel in the 
mask such that each MRF neighborhood has a non-repeating code label set.  
Thus each masked voxel in the ICM code image is assigned a value in the 
range $\{1,\ldots,C\}$ where $C$ is the maximum code label.
Such an image can be created and viewed with Atropos by assigning a valid
filename in the \texttt{--icm [1]} set of options.  An example adult brain
slice and the associated code image is given in 
Figure \ref{fig:r16icm} for an MRF neighborhood of $5 \times 5$ pixels.
This produces a maximum code label of `13'.
For each iteration, one has the option to permute the
set $\{1,\ldots,C\}$ which prescribes the order in which the
voxel labels are updated asynchronously. After the first pass through the
set of code labels, additional passes can further increase the posterior
probability until convergence (in $\sim$5 iterations).  One can specify
a maximum number of these ``ICM iterations'' on the command line.  For
our example in Figure \ref{fig:r16icm}, this means that for each
ICM iteration, we would iterate through the image 13 times only updating
those segmentation labels associated with the current ICM code.}}

\begin{figure}
\begin{center}
\begin{tabular}{c}
\includegraphics[width=160mm]{ICMfigure.pdf}
%\includegraphics[width=70mm]{r16slice.png} &
%\includegraphics[width=70mm]{r16icm.png} \\
%(a) & (b) \\
\end{tabular}
\end{center}
\caption{\baselineskip 12pt \small {\color{blue}{An adult brain image slice is shown 
with its ICM code image corresponding to a $5\times5$ MRF neighborhood.  
To the right of the ICM code image, we focus on a single neighborhood
with a center voxel associated with the ICM code label of '10'.  Each 
center voxel in a specified neighborhood exhibits a unique ICM code
label which does not appear elsewhere in its neighborhood.  When performing
the segmentation labeling update for ICM, we iterate through the set
of ICM code labels and, for each code label, we iterate through the image
and update only those voxels associated with the current code label.}}
  }
\label{fig:r16icm}
\end{figure}


% {\bf \textcolor{red}{ FIXME more specific?}}.
\item[Parameter Update (M-Step):] 
Note that the posteriors used in the previous iteration are used to 
estimate the parameters at the current iteration.  
We use a common and elementary estimate of the mixing parameters:
\begin{align}
\gamma_k \leftarrow \frac{1}{N} \sum_{i=1}^N  p_k(l_k|y_i).
\end{align}
We update the parametric model parameters by computing, for each of
  $K$ labels, the mean,
\begin{align}  
 \mu_k \leftarrow \frac{ \sum_{i=1}^N  y_i p_k(l_k|y_i)}{ \sum_{i=1}^N p_k(l_k|y_i) }
\end{align}
and variance,
\begin{align}  
 \sigma^2_k \leftarrow  \frac{ \sum_{i=1}^N  (y_i - \mu_k )^T
   p_k(l_k|y_i) (y_i - \mu_k )}{ \sum_{i=1}^N p_k(l_k|y_i) }.
\end{align}
The latter two quantities are modified, respectively, in the case of multivariate data as follows:
\begin{align}
 \boldsymbol{\mu}_k \leftarrow \frac{ \sum_{i=1}^N  \mathbf{y}_i p_k(l_k|\mathbf{y}_i)}{ \sum_{i=1}^N p_k(l_k|\mathbf{y}_i) }
\end{align}
and the $k^{th}$ covariance matrix, $\boldsymbol{\Sigma}_k$, is calculated from
\begin{align}
 \boldsymbol{\Sigma}_{k} \leftarrow \frac{ \sum_{i=1}^N  p_k(l_k|\mathbf{y}_i) ( \mathbf{y}_{i} - \boldsymbol{\mu}_{k} )^{\mathrm{T}}  ( \mathbf{y}_{i} - \boldsymbol{\mu}_{k} )}{1 - \sum_{i=1}^N p^2_k(l_k|\mathbf{y}_{i}) }.
\end{align}
This type of update is known as {\em soft} EM.  Hard EM, in
contrast, only uses sites containing label $l_k$ to update the 
parameters for the $k^{th}$ model.
A similar pattern is used in non-parametric cases.
\end{description}

EM will iterate toward a local maximum.  We track convergence by
summing up the maximum posterior probability at each site over the
segmentation domain.  The E-step, above, depends upon the selected
coding strategy \citep{Besag1986}.  Atropos may use either a
classical, sequential checkerboard update or a synchronous update of
the labels, the latter of which is commonly used in practice.
Synchronous update does not guarantee convergence but we employ it by
default due to its intrinsic parallelism and speed.  The user may
alternatively select checkerboard update if he or she desires
theoretical convergence guarantees.  However, we have not identified
performance differences, relative to ground truth, that convince us of
the absolute superiority of one approach over the other.
%\begin{align}
%P(l_k | y_i, t_i, m_i,d_i) &= \frac{P(y_i|l_k ,t_i,m_i,d_i)   P(l_k|t_i,m_i,d_i)   }{P(y_i|t_i,m_i,d_i)}
%\end{align}
%
%
%An image, $I$, maps a domain, $\Omega$ into the
%positive real numbers, such that $I \colon \Omega \rightarrow
%\mathbb{R}^+$.  The goal of segmentation, in general, is to define the
%spatial distribution of a finite set of labels over this domain.  We
%denote the segmentation itself as $\eta \colon \Omega \rightarrow L$
%where $L = \{ L_1 = 1 , \cdots , L_N=N \}$, a set of integer indexed
%segmentation labels.  Note that $\eta$ may be formed as $\eta =
%\sum_{i=1}^{i=N} L_i \eta_i $ where $\eta_i$ is the binary
%segmentation for label $L_i$.  A prior estimate for the label image
%$\eta_i$ is here denoted $\eta^s_i$ with a complete set of priors
%denoted $\eta^s$.  The boundary of the binary segmentation -- where a
%$0/1$ transition edge exists -- is denoted $\partial \eta^s_i$, for
%the prior, and $\partial \eta_i$ for the label image.
%
%\subsection{Apocrita Theory} A general maximum a posteriori criterion
%for segmentation seeks,
%\begin{equation} {\hat \eta} = \argmax _{\eta} \Pr( \eta | I )(\x) =
%\Pr( I | \eta )(\x) \Pr(\eta)(\x),
%\end{equation} 
%where $I$ is the input image, ${\eta}$ represents the
%label set configuration taken from the set $L$, $\Pr$ is the
%probability, $\x$ is the spatial index and the optimal solution is
%$\hat \eta$.  The input image $I$, here, is an unlabeled T1 MRI
%indexed by the value $\x \in \Omega$ where $\Omega$ is the image's
%spatial domain.  This probability is composed of the likelihood (first
%term) and the prior (second term).  Atropos ~ uses a spatially varying
%likelihood term and a two-component prior term that takes into account
%both spatial distribution and label smoothness, the latter via a
%standard MRF prior.
%
%The likelihood term for a single label value $L_i \in L$ is,
%\begin{eqnarray} \Pr( I | \eta_i )(\x)=\frac{1}{Z_i} \exp(- \| I(\x) -
%\mu_i(\x) \|^2 / \sigma_i^2 ),
%\end{eqnarray} where $Z_i$ is a normalizing constant, $\mu(\x)$ is a
%spatially varying estimate of the tissue mean and $\sigma$ is a
%standard deviation.  The prior term is given by,
%\begin{eqnarray} \Pr(\eta_i)(\x) = \frac{1}{Z^\prime_i} \exp( -f( \x -
%\y_{\partial \eta^s_i} ) / \sigma_{\eta^s_i}^2 ) p(\eta_i |
%\eta^\aleph_i), \\ \notag f( \x - \y_{\partial \eta^s_i} ) = (1 -
%\eta^s_i (\x) ) \| \x - \y_{\partial \eta^s_i} \|,
%\end{eqnarray} where $p(\eta_i | \eta^N_i)$ is the MRF smoothness
%probability based on the local neighborhood $\aleph$, the $Z$ is a
%normalizing constant and $\y_{\partial \eta^s_i}$ is the nearest point
%to $\x$ on the boundary of this labeling. Atropos ~requires a user or
%template-defined $\eta^s_i$ and the standard deviation
%$\sigma_{\eta^s_i}$ if a non-unity spatial prior component is desired
%for that label.  The free parameters, that must be estimated
%iteratively, are therefore $\mu_i, \sigma_i$ and the label set itself
%$\hat \eta$, which defines the (locally) optimal spatial distribution
%of $L$ through $\Omega$.  Figure~\ref{fig:spatp} shows the
%distribution of the spatial prior as a function of the distance from
%prior-defined object boundary.  Note that $f$ may easily be varied for
%other applications or that fixed probability images may also be
%substituted here.  This choice of $f$ is motivated by the fact that it
%allows compressed storage of the priors in a single image, $\eta^s$,
%while also maintaining the ability to manipulate -- for each
%$\eta^s_i$ -- the spatial influence of the prior via
%$\sigma_{\eta^s_i}$.  Practically, this is especially valuable when,
%$N$, the number of labels, is large.


\section{Implementation} 
Organization of the implementation section roughly follows that of the
theory section.  
%Thus, the reader can refer to, for instance, 2.2 for
%the likelihood theory and 3.3 for likelihood implementation.
\begin{figure}
\begin{center}
\begin{tabular}{c}
\includegraphics[width=160mm]{AtroposFlowchart.pdf}
\end{tabular}
\caption{\baselineskip 12pt \small Flowchart illustrating Atropos usage typically beginning with bias correction via N4.  
Initialization provides an estimate before the iterative optimization in which the 
likelihood models for each class are tabulated from the current estimate followed by a recalculation of the 
posterior probabilities associated with each class.  The multiple options associated with the different algorithmic 
components are indicated by the colored rounded rectangles connected to their respective core Atropos 
processes via curved dashed lines.  }
\label{fig:flowchart}
\end{center}
\end{figure}
\subsection{The Atropos User Interface}
As with other classes that comprise ANTs, Atropos uses the Insight
Toolkit as a developmental foundation.  This allows us to take
advantage of the mature portions of ITK (e.g. image IO) and ensures
the integrity of the ancillary processes such as those facilitated by
the underlying statistical framework.  Although Atropos is publicly
distributed with the rest of the ANTs package, we plan to contribute
its core elements to the Insight Toolkit where it can be vetted and improved by other interested researchers.

An overview of Atropos components can be gleaned, in part, from the flowchart depicted in 
Fig.~\ref{fig:flowchart}.  {\color{blue}{Given a set of input images and a mask images, each is preprocessed using N4 to correct for intensity inhomogeneity.  For our brain processing pipeline, the mask is usually obtained from the standard skull-stripping preprocessing step which also uses Atropos.  Initialization can be performed in various ways using standard clustering techniques, such as K-means, to prior-based images.  This initialization is used to provide the initial estimate of the parameters of the likelihood model for each class.  These likelihoods combine with the current labeling to generate the current estimate of the posterior probabilities at each voxel for each class.  At each iteration, one can also integrate N4 by using the current posterior probability estimation of the white matter to update the estimate of bias field. }}

To provide a more intuitive interface without the overhead costs of a graphical user interface, a set of unique command line parsing classes were developed which can also provide insight to the functionality of Atropos.  
The short version of the command line help menu is given in Listing
\ref{listing:command} which is invoked by typing `{\ttfamily Atropos
  -h}' at the command prompt.  Both short and long option flags are
available and each option has its own set of possible values and
parameters introduced in a more formal way in both the previous discussion and related papers cited in the introduction.  Here we describe these options from the unique perspective of  implementation.

\singlespacing 
\begin{command}
\lstsetcpp
\begin{lstlisting}
COMMAND: 
     Atropos

OPTIONS: 
     -d, --image-dimensionality 2/3/4
     -a, --intensity-image [intensityImage,<adaptiveSmoothingWeight>]
     -b, --bspline [<numberOfLevels=6>,<initialMeshResolution=1x1x...>,<splineOrder=3>]
     -i, --initialization Random[numberOfClasses]
                          KMeans[numberOfClasses]
                          Otsu[numberOfClasses]
                          PriorProbabilityImages[numberOfClasses,
                            fileSeriesFormat(index=1 to numberOfClasses) or vectorImage,
                            priorWeighting,<priorProbabilityThreshold>]
                          PriorLabelImage[numberOfClasses,labelImage,priorWeighting]
     -p, --posterior-formulation Socrates[<useMixtureModelProportions=1>,
                                   <initialAnnealingTemperature=1>,<annealingRate=1>,
                                   <minimumTemperature=0.1>]
                                 Plato[<useMixtureModelProportions=1>,
                                   <initialAnnealingTemperature=1>,<annealingRate=1>,
                                   <minimumTemperature=0.1>]
     -x, --mask-image maskImageFilename
     -c, --convergence [<numberOfIterations=5>,<convergenceThreshold=0.001>]
     -k, --likelihood-model Gaussian
                            HistogramParzenWindows[<sigma=1.0>,<numberOfBins=32>]
     -m, --mrf [<smoothingFactor=0.3>,<radius=1x1x...>]
     -g, --icm [<useAsynchronousUpdate=1>,<maximumNumberOfICMIterations=1>,
                <icmCodeImage=`'>]     
     -o, --output [classifiedImage,<posteriorProbabilityImageFileNameFormat>]
     -u, --minimize-memory-usage (0)/1
     -w, --winsorize-outliers BoxPlot[<lowerPercentile=0.25>,<upperPercentile=0.75>,
                                <whiskerLength=1.5>]
                              GrubbsRosner[<significanceLevel=0.05>,<winsorizingLevel=0.10>]
     -e, --use-euclidean-distance (0)/1
     -l, --label-propagation whichLabel[sigma=0.0,<boundaryProbability=1.0>]
     -h 
     --help 
\end{lstlisting} 
\end{command}

\doublespacing
\subsection{Initializing the Atropos Objective}
Atropos has a number of parameters defined within
Listing \ref{listing:n4} and will function on 2, 3 or 4 dimensional
data.  However, the majority of the time, users will be concerned with
a smaller set of input parameters.  Here, we list the recommended input and
an example definition for each parameter:
\begin{description}
\item[Input images to be segmented:] If more than
  one input image is passed, then a multivariate model will be
  instantiated.  E.g.  {\ttfamily -a Image.nii.gz} for one image and
  {\ttfamily -a Image1.nii.gz -a Image2.nii.gz} for multiple images.
\item[Input image mask:] This binary image
  defines the spatial segmentation domain.  Voxels outside the
  masked region are designated with the label $0$.  E.g.  {\ttfamily -x mask.nii.gz}. 
\item[Convergence criteria:] The algorithm terminates if it reaches  the maximum number of iterations or
  produces a change less than the minimum threshold change in the posterior.
  E.g. {\ttfamily -c [5,1.e-5]}. 
\item[MRF prior:] The key parameter to increase or decrease the
  spatial smoothness of the label map is $\beta$.  A useful range of
  $\beta$ values is $0$ to $0.5$ where we usually use 0.05, 0.1 or 0.2
  in brain segmentation.  E.g.  {\ttfamily -m [0.1, 1x1x1]} would
  define $\beta=0.1$ with a MRF radius of one voxel in each of three dimensions.
\item[Initialization:] The initialization options include (where the
  first parameter defines $K$, here 3 for each below),
\begin{itemize}
\item  {\ttfamily -i  Kmeans[3]} standard K-means initialization for
  three classes,
\item  {\ttfamily -i  PriorLabelImage[3,label\_image.nii.gz]} and
%  where scalar $w \in [0,1]$ is unused. 
\item  {\ttfamily -i  PriorProbabilityImages[3,label\_prob\%02d.nii.gz,w]} 
where $w=0$~(use the prior probability images only for
  initialization) or $w>0.0$ (use the prior probability images
  throughout the optimization).  If one chooses $0 < w < 1.0$ then one
  will increase (from zero) the weight on the priors.  These images,
  like the {\ttfamily PriorLabelImage}, should be defined with the same domain as
  the input images to be segmented.  
\end{itemize}
\item[Posterior formulation:]  The user may choose to estimate the
  mixture proportions (or not) by setting  {\ttfamily -p Socrates[1]}
  or {\ttfamily -p Socrates[0]}.  Fixed label boundary conditions may be employed by
  selecting the {\ttfamily PriorLabelImage} initialization and   {\ttfamily -p Plato[0]}.
\item[Output:] Atropos will output the hard segmentation and the
  probability image for each model.  E.g.  {\ttfamily -o
    [segmentation.nii.gz,seg\_prob\%02d.nii.gz]} will write
  out the hard segmentation in the first output parameter and a
  probability image for each class named, here, seg\_prob01.nii.gz,
  seg\_prob02.nii.gz, etc. 
\end{description}
Higher dimensions than 4 are possible although we have not yet encountered such 
an application-specific need.  Multiple images (assumed to be of the same
dimension, size, origin, etc.), will automatically enable multivariate
likelihoods.  In that case, the first image specified on the command
line is used to initialize the Random, Otsu, or K-means labeling with
the latter initialization refined by incorporating the additional intensity images, i.e. an initial univariate K-means clustering is
determined from the first intensity image which, along with the other
images, provides the starting multivariate cluster centers for a
follow-up multivariate K-means labeling.  More details on each of the
key implementation options are given below.

\subsection{Likelihood Implementation}
As mentioned previously in the introduction, different groups have
opted for different likelihood models which have included either
parametric (Gaussian) or non-parametric variations.
However, these approaches are similar in that they require a list
sample of intensity data from the input image(s) and a list of
weighting values for each observation of the list sample from which
the model is constructed.  In general, one may query model
probabilities by passing a given pixel's single intensity (for
univariate segmentation) or multiple intensities (for multivariate
segmentation) to the modeling function, regardless of whether the
function is parametric or non-parametric.  These similarities permitted
the creation of a generic plug-in architecture where classes
describing both parametric and non-parametric observational models are
all derived from an abstract list sample function class.  Three
likelihood classes have been developed, one parametric and two
non-parametric, and are available for usage although one of the
non-parametric classes is still in experimental development.  The
plug-in architecture even permits mixing likelihood models with
different classes during the same run for a hybrid
parametric/non-parametric model although this possibility has yet
to be fully explored.

If the Gaussian likelihood model is chosen, the list sample of
intensity values and corresponding weights comprised of the posterior
probabilities are used to estimate the Gaussian model parameters, i.e.
the mean and variance.  For the non-parametric model, the list sample
and posteriors are used in a Parzen windowing scheme on a weighted
histogram to estimate the observational model \citep{Awate2006}.

\subsection{Prior Probability Models}
\subsubsection{Label Regularity}
Consistent with our previous discussion, we offer both an MRF-based
prior probability for modeling spatial coherence and the possibility
of specifying a set of prior probability maps or a prior label map
with the latter extendable to creating a dense labeling.  To invoke
the MRF `{\ttfamily -m/--mrf}' option, one specifies the smoothing
factor (or the granularity parameter, $\beta$, given in
Eqn. (\ref{eq:U}), and the radius (in voxels) of the neighborhood
system using the vector notation `{\ttfamily 1x1x1}' for a
neighborhood radius of 1 in all 3 dimensions.  This radius is defined
such that voxels including but not limited to those that are face-connected will influence the MRF.  

\begin{comment}{
The user also has an option to propagate initial labels, set by the
prior options, by a distance map tool.  
To propagate labels with the specification of prior probability images
or a prior label image, one needs to select the distance function (set
by the `{\ttfamily -e/--use-euclidean-distance}' boolean option)
which, by default, is determined by the geodesic distance.  The user
also needs to set the label propagation parameters for one or more of
the classes.   This  includes both the boundary probability and the exponential
 decay parameter, respectively $\alpha_k$ and $\sigma_k$ in Eqn. (\ref{eq:prop}).  
}\end{comment}
\doublespacing
\subsubsection{Registration and Probability Maps} 
Image registration enables one to transfer information
between spatial domains which may aid in both segmentation and bias
correction.  We rely heavily on template-building strategies
\citep{Avants2010,Avants2010a} which are also offered in ANTs.  Since
aligned prior probability images and prior label maps are often
associated with such templates, Atropos can be initialized with these
data with their influence regulated by a prior probability weighting
term.  Although prior label maps can be specified as a single
multi-label image, prior probability data are often represented as
multiple scalar images with a single image corresponding to a
particular label.  For relatively small classifications, such as the
standard 3-tissue segmentation (i.e. white matter, gray matter, and
cerebrospinal fluid), this does not typically present computational
complexities using modern hardware.  However, when considering dense
cortical parcellations where the number of labels can range upwards of
74 per hemisphere \citep{Destrieux2010}, the memory load can be
prohibitive if all label images are loaded into run-time memory
simultaneously.  A major part of minimizing memory usage in Atropos,
which corresponds to the boolean `{\ttfamily
-u/--minimize-memory-usage}' option, is the sparse representation of
each of the prior probability images.  Motivated by the observation
that these spatial prior probability maps tend to be highly localized
for large quantities of cortical labels, a threshold is specified on
the command line (default = 0.0) and only those probability values
which exceed that threshold are stored in the sparse representation.
During the course of optimization, the prior probability image for a
given label is reconstructed on the fly as needed.  For instance, the
NIREP (www.nirep.org) evaluation images are on the order of $300
\times 300 \times 256$ with 32 cortical labels.  Our novel memory
minimizing image representation typically shrinks run-time memory usage from a
peak of 10+ GB to approximately 1.5 GB and enable these datasets to be
used for training/prior-based cortical parcellation.


\singlespacing 
\begin{command}
\lstsetcppnfour
\begin{lstlisting}
COMMAND: 
     N4BiasFieldCorrection

OPTIONS: 
     -d, --image-dimensionality 2/3/4
     -i, --input-image inputImageFilename
     -x, --mask-image maskImageFilename
     -w, --weight-image weightImageFilename
     -s, --shrink-factor 1/2/3/4/...
     -c, --convergence [<numberOfIterations=50>,<convergenceThreshold=0.001>]
     -b, --bspline-fitting [splineDistance,<splineOrder=3>,<sigmoidAlpha=0.0>,
                           <sigmoidBeta=0.5>]
                           [initialMeshResolution,<splineOrder=3>,<sigmoidAlpha=0.0>,
                           <sigmoidBeta=0.5>]
     -t, --histogram-sharpening [<FWHM=0.15>,<wienerNoise=0.01>,<numberOfHistogramBins=200>]
     -o, --output [correctedImage,<biasField>]
     -h 
     --help 
\end{lstlisting} 
\end{command}
\doublespacing
\subsubsection{Integrating N4 Bias Correction}
Assumptions about bias correction may be thought of as another prior
model.  As such, the typical segmentation processing pipeline begins with an intensity
normalization/bias correction step using a method such as the recently developed N4
algorithm \citep{Tustison2010}.  {\color{blue}{N4 extends the popular
nonparametric nonuniform intensity normalization (N3) algorithm \citep{Sled1998}
in two principal ways:
\begin{itemize}
  \item We replace the least squares B-spline fitting with a parallelizable alternative 
  (which we also made publicly available in the Insight Toolkit)---
  the advantages being that 1) computation is much faster 
  and 2) smoothing is not susceptible to outliers as is characteristic with
  standard least squares fitting algorithms.
  \item The fitting algorithm permits a multi-resolution approach so whereas standard 
  N3 practice is to select a single resolution at which bias correction occurs, the N4
  framework permits a multi-resolution correction where a base resolution is chosen and
  correction can then occur at multiple resolution levels each resolution being twice 
  the resolution of the previous level.
\end{itemize}  
Specifically, with respect to segmentation, there exists a third advantage with N4 over
N3 in that the former permits the specification of a probabilistic mask as opposed 
to a binary mask.}}
%It was previously mentioned that
%several methods proposed in the literature have taken an integrative
%view of the segmentation problem by incorporating an intrinsic bias
%correction step into the actual workflow.  The
%advances introduced with N4 permit such an adaptive
%integration with Atropos.  
Recent demonstrations suggest improved
white matter segmentation produces better gain field estimates using
N3 \citep{Boyes2008}.  Thus, when performing 3-tissue segmentation, we
may opt to use, for instance, the posterior probability map of white matter at the current
iteration as a weighted mask for input to N4.  This is done by setting
the `{\ttfamily --weight-image}' option on the N4 command line call
(see Listing \ref{listing:n4}) to the posterior probability image
corresponding to the white matter produced as output in the Atropos
call, i.e. `{\ttfamily Atropos --output}'.  N4 was recently added  
to the Insight Toolkit repository%
\footnote{
http://www.itk.org/Doxygen/html/classitk\_1\_1N4MRIBiasFieldCorrectionImageFilter.html
} 
where it is built and tested on
multiple platforms nightly.  The
evaluation section will illustrate inclusion of Atropos, N4 and ANTs
in a brain processing pipeline.  

\subsection{Running the Atropos Optimization} 
The Atropos algorithm is cross-platform and compiles on, at minimum,
modern OSX, Windows and Linux-based operating systems.  The
user interface may be reached through the operating system's user
terminal.  Because of its portability and low-level efficiency,
Atropos can easily be called from within other packages, such as
Matlab or Slicer, or, alternatively, integrated at compile time as a
library.  A typical call to the algorithm, illustrated here with ANTs
example data, is:
{\ttfamily Atropos -d 2 -a r16slice.nii.gz -i kmeans[3] -c [5,0] -x
mask.nii.gz -m [0.2,1x1] -o
[r16\_seg.nii.gz,r16\_prob\_\%02d.nii.gz]}.  In this case, Atropos
will output the segmentation image, the per-class probability images
and a listing of the parameters used to set up the algorithm.  A
useful feature is that one may re-initialize the Atropos EM via the
{\ttfamily -i PriorProbabilityImages[...]} option.  This feature
allows one to compute an initial segmentation via K-means, alter the
output probabilities by externally computed
functions (e.g. Gaussian smoothing, image similarity or edge maps) and re-estimate the
segmentation with the modified priors.  Finally, the functionality
that is available to parametric models is equally available to the
non-parametric models enabled by Atropos.

\section{Evaluation}
Atropos~encodes a family of segmentation techniques that may be
instantiated for different applications but here we evaluate only two
of the many possibilities. First, we perform an evaluation on the
BrainWeb dataset using both the standard T1 image with multiple bias
and noise levels and also the BrainWeb20 data
\citep{Aubert-Broche2006a,Battaglini2008}.  In combination, these data
allow one to vary not only noise and bias but also the underlying
anatomy.  Second, we evaluate the use of Atropos in improving whole-brain parcellation and exercise its ability to efficiently solve {\em
many-class} expectation maximization problem.  We choose this
evaluation problem in part to illustrate the flexibility of Atropos
and also the benefits of the novel, efficient implementation that
allows many-class problems to be solved with low memory usage ($<$2GB
for a 69-class model on 1 mm$^3$ brain data).


\subsection{BrainWeb Evaluation}
\label{sec:bweb} The BrainWeb data is freely available at
\url{http://mouldy.bic.mni.mcgill.ca/BrainWeb/}.  We employ both the
individual subject data and the BrainWeb20 data in this evaluation.
\subsubsection{Single-Subject Evaluation} We use the single-subject
data with 3\% noise and three levels of bias referred to as 0, 20 and 40\% RF inhomogeneity.  We
study the effect of the MRF prior term and initialization on
the Dice overlap between ground truth and the segmentation result for
each tissue.  We test both K-means and prior label image
initialization with MRF $\beta \in \{ 0.00 , 0.05 , 0.10 , 0.15 , 0.20, 
0.25 , 0.30 \}$ at each bias field.  We also feed the white matter
probability map derived from K-means into N4 to guide the bias
correction.%
\footnote{
A comprehensive evaluation of N4 reported in \cite{Tustison2010} used the BrainWeb data set
to compare performance with the original N3 algorithm \citep{Sled1998}.
}
Segmentation is then repeated, with the same parameters,
but with the N4-corrected image as input.  The resulting algorithm is
similar to those that fix segmentation parameters while
estimating bias and fix bias while estimating segmentation parameters.
Thus, with this simple evaluation, we are able to compare the impact
of bias on the combination of N4 and Atropos and also the validity of our
prior label image initialization.  
% The prior label image manual initialization was based on a crude
% ITK-SNAP labeling of a small set of voxels for each tissue.  
Results of these evaluation scenarios, in terms of
Dice overlap, are shown in Figure~\ref{fig:bweb1}. Because overlap
ratios with N4 bias correction approximate those of the zero bias data, we may
conclude that simple N4 pre-processing is adequate to correct even the 40\%
RF bias level.  An example of this procedure, using BrainWeb data with 40\% RF bias,
is in Figure~\ref{fig:bwebrf40}.  We supply the information necessary
to repeat the results in this figure in the script entitled
`{\ttfamily atroposBwebRF40FigureExample.sh}' which is available in
the ANTs Atropos documentation folder as of SVN commit 711.
The script may be easily modified to run the whole
evaluation.  Figure~\ref{fig:bwebrf40} shows the results of
simultaneously using proton density and T1-weighted BrainWeb data to
perform the segmentation.  This multivariate input data outperforms
the univariate T1-weighted data alone.
\begin{figure}
\begin{center}
\begin{tabular}{c}
\includegraphics[width=6in]{Figures/bwebN4ex.pdf}
\end{tabular}
\caption{\baselineskip 12pt \small We combine N4 and Atropos by simple
sequential processing and apply to BrainWeb T1-weighted single-subject
data with 40\% RF bias and 3\% noise.  The $\beta$ for the MRF term
is, here, $0.2$.  Slice 71 of the input data is in (a).  The initial
K-means ($K=3$) segmentation is in panel (b).  We use the brain mask
to guide N4 bias correction and produce the image in (c).  We
repeat the K-means segmentation, but with the N4-corrected image as
input and produce the segmentation in (d).  The average 3-tissue Dice
overlap of result (b) is 0.906 while the average overlap for (d) is
0.954.  Arrows highlight a region of large before-after segmentation
discrepancy.  In (e) we see the BrainWeb proton density image with no
inhomogeneity and 3\% noise.  Its segmentation is in (f) with average
3-tissue Dice overlap of 0.895.  In (g) we use both proton density
data and T1 data as multiple modality input to Atropos.  The segmentation
of this two-modality input data, using a multivariate Gaussian model,
produces average 3-tissue Dice overlap of 0.958, which exceeds
the univariate solution.  An arrow highlights one region where there
is small, visually recognizable improvement in sulcal segmentation
relative to the result from T1 data alone.  A second area of
improvement is the putamen segmentation.  The ground truth
segmentation is in (h).  The multivariate segmentation result, in
combination with the low PD segmentation performance, suggests PD and
T1 provide complementary information that may improve 3-tissue
segmentation and serves to validate the multivariate Atropos
implementation.  In this case, the benefit is likely to derive from
the fact that the PD image has no bias.
}
\label{fig:bwebrf40}
\end{center}
\end{figure}

\begin{figure}
\begin{center}
\begin{tabular}{c}
\includegraphics[height=2in]{Figures/tissue_dice_vs_mrf_B_RF0.pdf}
\includegraphics[height=2in]{Figures/tissue_dice_vs_mrf_B_RF20.pdf}
\includegraphics[height=2in]{Figures/tissue_dice_vs_mrf_B_RF40.pdf}\\
\includegraphics[height=2in]{Figures/tissue_dice_vs_mrf_N4_RF0.pdf}
\includegraphics[height=2in]{Figures/tissue_dice_vs_mrf_N4_RF20.pdf}
\includegraphics[height=2in]{Figures/tissue_dice_vs_mrf_N4_RF40.pdf}
% \includegraphics[height=2in]{Figures/tissue_dice_vs_mrf_PLI_RF0.pdf}
% \includegraphics[height=2in]{Figures/tissue_dice_vs_mrf_PLI_RF20.pdf}
% \includegraphics[height=2in]{Figures/tissue_dice_vs_mrf_PLI_RF40.pdf}
\end{tabular}
\caption{\baselineskip 12pt \small BrainWeb single-subject results for
  each tissue.  The results show that N4 bias correction, combined with Atropos, results in a minimal effect of
  bias, even at the 40\% level.  The optimal $\beta$ for the MRF term
  appears to be between $0.1$ and $0.2$.  
% The prior label image initialization is competitive with but inferior to K-means.  This is
%  sensible in that the prior labels image outcome is sensitive to the
%  user choices made.  In this study, we labeled the brain web template
%  very sparsely, quickly and crudely and did not expect highly
%  accurate results.  
  The legend is in the same position in each graph, allowing a visual
  comparison of the results.  As one may see, the N4-assisted overlap
  values are consistent across bias field/RF inhomogeneity.}
\label{fig:bweb1}
\end{center}
\end{figure}
\subsubsection{20-subject Evaluation} The single-subject BrainWeb
study in the previous section tested the basic Atropos options and the
benefit of N4 for segmentation in the presence of bias.  The 20
subject BrainWeb data allows us to use 2-fold cross-validation to test
our ability to segment different individuals reliably.  In this study,
we divide the 20 subjects equally into training and testing groups.
We then exploit the ground-truth labeling of the training data to
build both a group template \citep{Avants2010} and also prior
probability maps for each of the three major tissues in the cerebrum.
Each prior probability map is gained by deforming the ground truth
labels from each of the 10 training subjects to their template and
averaging component by component.  We then deform the template---and
priors---to the ten testing subjects and run Atropos with not only
{\ttfamily KMeans[3]} initialization but also {\ttfamily
  PriorProbabilityMap[3,priors\%02d.nii.gz,$w$] } where $w \in
\{0.0,0.5\}$.  We then switch the roles of testing and training sets to
gain 3-tissue segmentation for each of the twenty subjects.  
When $w=0$, the priors are only used in initializing
the model parameters but not during subsequent EM iterations.  
When $w=0.5$, the priors are maintained in the product with the
likelihood during all EM iterations.  Results, in terms of bar
plots for Dice overlap mean and standard deviation, are shown in Figure~\ref{fig:bweb20}.
\begin{figure}
\begin{center}
\begin{tabular}{c}
\includegraphics[width=2.2in]{Figures/bw20_kmeans.pdf}
\includegraphics[width=2.2in]{Figures/bw20_priorIni.pdf}
\includegraphics[width=2.2in]{Figures/bw20_priorSeg.pdf}
\end{tabular}
\caption{\baselineskip 12pt \small BrainWeb 20-subject results for
  each tissue as a function of MRF-$\beta$ parameter where MRF-$\beta$
  is in $\{0,0.05,0.1,0.15,0.2\}$ and increases left to right.  The results show that the PriorProbabilityMaps with
  $w=0.5$ (far right) gives the best performance for all tissues.}
\label{fig:bweb20}
\end{center}
\end{figure}



\subsection{The Hammers Dataset Evaluation}
\label{sec:hammers}
We evaluate the ability to improve multi-template labeling results by
converting the group labels to probability maps and using them to
drive many-class EM segmentation.  The ground truth labels cover 69
classes and much of the brain.  Some unlabeled regions remain which we
assign to label 69 such that all brain parenchyma contains a unique
label.  Following \cite{Avants2010a}, the initialization of our
evaluation applies the script {\ttfamily ants\_multitemplate\_labeling.sh }
(available in ANTs) to the 19 Hammers evaluation datasets
located at \url{http://www.brain-development.org/} and currently under
the adult atlases section 
\citep{Hammers2003,Heckemann2006}.  These initial majority voting
results are competitive with prior work
\citep{Heckemann2006,Heckemann2010} and serve as a baseline against
which we compare.

We first convert each of the 69 labels within the original evaluation
dataset to an individual image.  The remaining steps,
summarized briefly, are the same for each of the 19 subjects.  We
select one subject as an unlabeled target.  The other 18 datasets are
then mapped (as in the script above) to that subject.  We then deform,
individually, the 69 $\times$ 18 label images to the unlabeled
subject.  The label probability map is gained by averaging the 18
deformed images associated with each label.  We repeat this for each
subject.  The following parameters are the most relevant to this discussion:
{\ttfamily  -i  PriorProbabilityImages[69,label\_prob\%02d.nii.gz,0.5] 
-m [0.2,1x1x1] -c [5,0] -p Socrates[1]}.  Results, in terms of
Dice overlap, are shown in Figure~\ref{fig:hammers}.
\begin{figure}
\begin{center}
\begin{tabular}{c}
\includegraphics[height=8in]{Figures/final_results_majority_vote_vs_atropos.pdf}
\end{tabular}
\caption{\baselineskip 12pt \small The figure compares the Dice
overlap results from Atropos versus the raw results from majority
voting for each of 68 neuroanatomical regions and, in addition, the
unlabeled portions of the brain from the Hammers evaluation dataset.
We evaluated Atropos via N-fold cross-validation and employed
PriorProbabilityImages for each class where probabilities are gained
by averaging mapped subject labels. The color coding highlights those
regions that have the highest (yellow) and lowest (pink) improvement.
The significance of the improvement, measured by pairwise T-test, is
also shown as is a trinary coding of that improvement as: $+$
significant improvement, $-$ performance reduction, $\sim$ no change.}
\label{fig:hammers}
\end{center}
\end{figure}

\subsection{Reproducibility of this Evaluation}
\noindent{\bf Data:} The BrainWeb data is freely available.  We used
single-subject BrainWeb data as is but added a metaformat data header
to the raw binary files.  An example copy of this header is contained
in {\ttfamily atroposBwebRF40FigureExample.sh }.  The 20-subject data,
however, required excluding non-cerebrum tissue classes.  The Hammers
data was also used as is (\url{http://www.brain-development.org/}).

\noindent{\bf Software:} The ANTs software is available at \url{http://www.picsl.upenn.edu/ANTs} with download and compilation
instructions at \url{http://www.picsl.upenn.edu/ANTS/download.php}.
SVN release 711 was used for the examples and evaluations performed in
this paper.  Some components of ANTs depend on the Insight ToolKit.
The most critical dependency, for Atropos, is the ITK statistics framework
used to implement the univariate and multivariate parametric models.
We linked to the git version of ITK current as of Dec. 1, 2010.  See
\url{http://www.itk.org/Wiki/ITK/Git/Download} for instructions on git ITK.

\noindent{\bf Scripts:} The complete script for the single-subject
BrainWeb study is based on generalizing {\ttfamily atroposBwebRF40FigureExample.sh}, which is available in the ANTs
toolkit (SVN release 711 or greater) and which reproduces Figure 2
results.  The template-based normalization procedure for the BrainWeb
20 and the Hammers evaluation data is based on freely available
scripts included with ANTs, {\ttfamily
  ants\_multitemplate\_labeling.sh} and {\ttfamily
  buildtemplateparallel.sh}.  A release version of ANTs---with a final version of Atropos---will
be prepared with the final version of this manuscript.

\section{Discussion}
We introduced Atropos, the theory and implementation
details and documented its performance in a variety of use cases.  We
also showed evidence that the openly available N4 bias correction can
easily be used with Atropos to improve segmentation.
Furthermore, we used multiple subject BrainWeb data to build dataset-specific priors that provided the most consistent segmentation
performance across tissues.  Finally, we used majority voting to
initialize an Atropos EM solution to a 69-class brain parcellation
problem.  Significant improvements were gained in multiple brain
regions, in particular in temporal lobe cortex, the hippocampi and
amygdalae and the lateral ventricles.  This work, in summary, proves
the applicability of Atropos in both basic and extended use cases.

\subsection{Performance on BrainWeb Data}
Atropos results are competitive with the state of the art.  For
instance, \cite{Ashburner2005} (SPM5) evaluated on 0\% RF bias field, 3\%
noise BrainWeb single subject data finding 0.932 (GM) and .961 (WM)
Dice overlap. Results on 40\% RF bias were 0.934 (GM) and 0.961 (WM). SPM5 exhibits
insensitivity to bias similar to our own best results on the 40\% RF bias, 3\% 
noise case (MRF-$\beta$=0.2, K-means $+$ N4) with Dice overlap for GM is
0.951 and for WM is 0.963.  \cite{Nakamura2009} gave GM Dice overlap
results (BrainWeb single 3\% noise) of 0.962 (0\% RF bias), 0.964
(20\% RF bias)
and 0.956 (40\% RF bias) which are slightly higher than either SPM5 or Atropos
results.  However, \cite{Nakamura2009} do not report WM or CSF numbers
for comparison.  Topology-preserving methods also perform well.
\cite{Shiee2010} achieved Dice overlap for 3\% noise 20\% RF bias BrainWeb
single subject with 0.912 (GM), 0.927 (WM) and 0.900 (CSF) Dice
overlap.  These are excellent numbers given the topological constraint applied to the segmentation.  \cite{Bazin2007c} proposed TOADS and, estimating from the paper's
graph, showed that the average Dice overlap accuracy for 3\% noise for various RF
was  0.930--0.950 (GM), 0.950--0.960 (WM), and 0.920--0.940 (CSF).
Perhaps the most recent balanced evaluation was performed in
\citep{Klauschen2009}, which reports confusion matrix numbers, rather
than Dice overlap.  Because the absolute true number of GM and WM
voxels for BrainWeb are known, we can convert the confusion matrix to
Dice overlap.  In that case, the SPM5 Dice overlap for BrainWeb
single-subject data is 0.885 (GM) and 0.909 (WM), while FreeSurfer and FSL's accuracy
is lower. The best GM Dice overlap result for the 20 subject BrainWeb data is obtained by SPM5:
0.930; the best WM Dice overlap is from FSL: 0.950.  We note that
\cite{Klauschen2009} used a comprehensive evaluation where quality of
brain extraction also contributed to the outcome.  Thus, the results
must be interpreted slightly differently than those from other papers.
Finally, in our evaluation of 20-subject BrainWeb data, the prior
probability models performed best of all the models used.  Compared to
the K-Means based segmentation, the prior based segmentation
performance also peaked at lower values of the MRF-$\beta$ term (0.0
and 0.05).  This is reasonable in that the spatial priors themselves
impose a degree of regularity on the segmentation, as in SPM5.

\subsection{Performance on Hammers Data}
Our prior work, \citep{Avants2011}, showed that the majority vote
initialization provided to Atropos by ANTs template mapping is
competitive with \cite{Heckemann2006}.  Overall, the Atropos EM
extension improved these results further.  However, in a few regions
of the mid-brain, the Atropos EM segmentation
performed significantly worse.  This is not surprising, in that
Atropos EM assumes that signal from the likelihood and MRF term is
valuable in improving the segmentation.  This assumption held for
amygdala and lateral ventricles among other areas.  However, in
pallidum and corpus callosum (the most significant areas with loss of
performance), this is not true.  We believe the explanation is that
the intensity varies within these structures and that a more complex
intensity model (or finer parcellation) would be needed here.  An
alternative solution would be to use boundary conditions for these
structures, as in the {\ttfamily PriorLabelImage} Atropos initialization option.

\subsection{Clinically-Related Evaluation}
While specifying performance on BrainWeb is highly
valuable, clinical validation is a second important aspect of
segmentation evaluation.  For instance, 
\citep{Freeborough1997,Westlye2009,Sanchez-Benavides2010,Chou2009,Bresser2011}
are only a few of the papers that evaluate segmentation performance with
respect to a known neurobiological outcome measure.  Atropos is
currently used in clinical studies and a number of clinically focused,
application-specific evaluations are ongoing and will constitute
future work.  One early example of a clinically-focused Atropos
neuroimaging application is in \citep{Avants2010c}.  A second successful
application area is that of ventilation-based segmentation of hyperpolarized helium-3 MRI
\citep{Tustison2010a} which also used the open source Glamorous Glue
algorithm to impose topology constraints \citep{Tustison2010b}.  Thus, future work may
incorporate topology more closely into the Atropos methodology.  

A more general advantage which extends beyond the scope of the experimental 
evaluation section of this paper is the flexibility of Atropos.   This includes 
not only $n$-tissue segmentation and dense volumetric cortical parcellation,
as reported in this work, but Atropos is also used in conjunction with our ANTs registration tools for robust
brain extraction which has reported good performance in comparison with other
popular, publicly available brain extraction tools \citep{Avants2010a}.  
 
\section{Conclusion}
The Atropos software is freely available to the public.  We release
this code not only to make it available to clinical researchers but
with the hope that other researchers in segmentation will provide
feedback about the implementation decisions that we made.  EM
segmentation is non-trivial and there are numerous design alternatives
available not only in the models selected but also in the ICM coding,
alternatives to ICM and the method in which prior and likelihood are
combined.  Due to the flexibility of Atropos, we also hope that some
of its capabilities, though not evaluated here, are explored by the segmentation or clinical community.

\paragraph{Information Sharing Statement}
{\color{blue}{Atropos software is available in ANTs
    \url{http://www.picsl.upenn.edu/ANTs} 
which depends on ITK \url{http://www.itk.org/Wiki/ITK/Git/Download}.
The data used in this work is available in the ANTs software
repository, BrainWeb 
\url{http://mouldy.bic.mni.mcgill.ca/BrainWeb/} and at \url{www.brain-development.org}.
We employed itk-SNAP for visualization \url{www.itksnap.org}.}}

\paragraph{Acknowledgments}
{This work was supported in part by NIH (AG17586, AG15116, NS44266, and
NS53488).}

\newpage

\bibliographystyle{neuroinformatics}
\bibliography{atropos} 

\end{document}

%# T1_RF40_3%_N4 reference results 
 Label 1 DICE 0.941891  RO 0.890165 TP1 0.904786 TP2 0.98217
 Label 3 DICE 0.966776  RO 0.935688 TP1 0.962814 TP2 0.97077
 Label 2 DICE 0.953862  RO 0.911794 TP1 0.971043 TP2 0.937279
 AvgDice  : 0.954176
%# T1_RF40_3%_N4 + T2_RF0_3% noise reference results 
Label 1 DICE 0.937508  RO 0.882367 TP1 0.889406 TP2 0.99111
 Label 3 DICE 0.96701  RO 0.936128 TP1 0.958398 TP2 0.975778
 Label 2 DICE 0.951639  RO 0.90774 TP1 0.977645 TP2 0.926981
 AvgDice  : 0.952052
%# T1_RF40_3%_N4 + PD_RF0_3% noise reference results 
 Label 1 DICE 0.944732  RO 0.895254 TP1 0.912413 TP2 0.979425
 Label 3 DICE 0.970348  RO 0.942405 TP1 0.969169 TP2 0.971531
 Label 2 DICE 0.957976  RO 0.919341 TP1 0.97094 TP2 0.945353
 AvgDice  : 0.957685
%# PD_RF0_3% noise reference results 
 Label 2 DICE 0.905959  RO 0.828085 TP1 0.892396 TP2 0.919941
 Label 3 DICE 0.942477  RO 0.891212 TP1 0.943874 TP2 0.941084
 Label 1 DICE 0.837847  RO 0.720944 TP1 0.87262 TP2 0.805739
 AvgDice  : 0.895428
%# T2 
 Label 1 DICE 0.931616  RO 0.871986 TP1 0.984176 TP2 0.884385
 Label 2 DICE 0.882997  RO 0.790505 TP1 0.79822 TP2 0.987921
 Label 3 DICE 0.901592  RO 0.820817 TP1 0.99405 TP2 0.82487
 AvgDice  : 0.905402
